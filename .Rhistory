#silhouette plot
library(cluster)
sil <- silhouette(cluster_output$cluster, dist(dataset.scaled))
fviz_silhouette(sil)
#silhouette average width is 0.29
#PCA
cor_val <- cor(dataset.cleared)
cor_val
mean(cor_val)
#perform PCA
pca <- prcomp(dataset.cleared, center = TRUE, scale = TRUE)
summary(pca)
#Eigenvalues
eigenvalues <- pca$sdev^2
#Eigenvectors
eigenvectors <- pca$rotation
eigenvalues
eigenvectors
#cumlative score
cumulative_scores <- cumsum(eigenvalues / sum(eigenvalues) * 100)
vehicles_transformed <- predict(pca, dataset.cleared)
colnames(vehicles_transformed) <- paste0("PC", 1:ncol(vehicles_transformed))
vehicles_transformed
pc_index <- min(which(cumulative_scores > 92))
pcs <- 1:pc_index
pcs
dataset.transformed <- as.data.frame(-pca$x[,1:6])
#NbClust
set.seed(23)
no_of_Cluster <- NbClust(dataset.transformed,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
rm(list=ls())
#Load the data set
#install.packages('xlsx')
#library(xlsx)
loadfile <- read.xlsx(file = 'vehicles.xlsx', sheetIndex = 1, header = TRUE)
#Load the data set
#install.packages('xlsx')
library(xlsx)
loadfile <- read.xlsx(file = 'vehicles.xlsx', sheetIndex = 1, header = TRUE)
Dataset <- loadfile[2:20]
#Detect Missing values
sum(is.na(Dataset))
#No missing data
#Detect outliers
boxplot(Dataset[1:18])
#Outliers founded columns
OutlierList <- c(
"Skew.maxis",
"Pr.Axis.Ra",
"Sc.Var.Maxis",
"Rad.Ra",
"Kurt.maxis",
"Skew.maxis",
"Max.L.Ra",
"Sc.Var.maxis"
)
#removing outliers
for (Var in names(Dataset)) {
if (Var %in% names(Dataset[, OutlierList])) {
outliers <- boxplot.stats(Dataset[,Var])$out
Dataset <- subset(Dataset, !Dataset[,Var] %in% outliers)
}
}
#Check still there is any outlier
boxplot(Dataset[1:18])
#there is out liers still and redo the process
#new outlier list
OutlierList2 <- c(
"Skew.Maxis",
"Sc.Var.maxis"
)
#removing remaining outliers
for (Var in names(Dataset)) {
if (Var %in% names(Dataset[, OutlierList2])) {
outliers <- boxplot.stats(Dataset[,Var])$out
Dataset <- subset(Dataset, !Dataset[,Var] %in% outliers)
}
}
#Check still there is any outlier
boxplot(Dataset[1:18])
dataset.cleared <- Dataset[1:18]
label <- Dataset$Class
#Scaling the Dataset
dataset.scaled <- scale(dataset.cleared)
#scaled dataset visualization
boxplot(dataset.scaled)
#Find the optimal number of clusters
#install.packages('factoextra')
#library(factoextra)
#install.packages('NbClust')
#library(NbClust)
#NbClust
set.seed(23)
no_of_Cluster <- NbClust(dataset.scaled,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
library(factoextra)
#install.packages('NbClust')
library(NbClust)
#NbClust
set.seed(23)
no_of_Cluster <- NbClust(dataset.scaled,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
#k =3
#Elbow method
fviz_nbclust(dataset.scaled, kmeans, method = 'wss')
#k =3
#silhouette method
fviz_nbclust(dataset.scaled, kmeans, method = 'silhouette')
#k =2
#Gap statistics
fviz_nbclust(dataset.scaled, kmeans, method = 'gap_stat')
#k =3
### Clustering Attempt k=3
cluster_output <- kmeans(dataset.scaled,3)
cluster_output
#total WSS
wss <- cluster_output$tot.withinss
wss
#BSS
bss <- cluster_output$betweenss
bss
#Clusters
table(label,cluster_output$cluster)
#Centers
cluster_output$centers
#install.packages('fpc')
library(fpc)
plotcluster(dataset.scaled,cluster_output$cluster)
#silhouette plot
library(cluster)
sil <- silhouette(cluster_output$cluster, dist(dataset.scaled))
fviz_silhouette(sil)
#silhouette average width is 0.29
#PCA
cor_val <- cor(dataset.cleared)
cor_val
mean(cor_val)
#perform PCA
pca <- prcomp(dataset.cleared, center = TRUE, scale = TRUE)
summary(pca)
#Eigenvalues
eigenvalues <- pca$sdev^2
#Eigenvectors
eigenvectors <- pca$rotation
eigenvalues
eigenvectors
#cumlative score
cumulative_scores <- cumsum(eigenvalues / sum(eigenvalues) * 100)
vehicles_transformed <- predict(pca, dataset.cleared)
colnames(vehicles_transformed) <- paste0("PC", 1:ncol(vehicles_transformed))
vehicles_transformed
pc_index <- min(which(cumulative_scores > 92))
pcs <- 1:pc_index
pcs
dataset.transformed <- as.data.frame(-pca$x[,1:6])
#NbClust
set.seed(23)
no_of_Cluster <- NbClust(dataset.transformed,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
#k =2
#Elbow method
fviz_nbclust(dataset.transformed, kmeans, method = 'wss')
#silhouette method
fviz_nbclust(dataset.transformed, kmeans, method = 'silhouette')
#Gap statistics
fviz_nbclust(dataset.transformed, kmeans, method = 'gap_stat')
dataset.cleared
View(dataset.cleared)
boxplot(dataset.transformed)
df_outliers <- boxplot(dataset.transformed, plot = FALSE)$out
# Remove rows of df_scaled where any element of each row matches any element of df_outliers
df_no_outliers <- dataset.transformed[!apply(dataset.transformed, 1, function(row) any(row %in% df_outliers)),]
boxplot(df_no_outliers)
set.seed(23)
no_of_Cluster <- NbClust(df_no_outliers,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
#k =2
fviz_cluster(new_kmeans,data = dataset.transformed)
new_kmeans <- kmeans(dataset.transformed,2)
new_kmeans
fviz_cluster(new_kmeans,data = dataset.transformed)
library(Metrics)
unnomarlize <- function(x,min,max){
return((max-min)*x+min)
}
#Actual result and predicted result table function
actual_predict_table <- function(actual,predicted){
act_pred_table <- cbind(actual,predicted)
colnames(act_pred_table) <- c("Expected","Predicted")
return(act_pred_table)
}
#Evaluation function
evaluateResult <- function(actual,pred){
rmse <- rmse(actual = actual,predicted = pred)
mae <- mae(actual = actual, predicted = pred)
mape <- mape(actual = actual, predicted = pred)
smape <- smape(actual = actual, predicted = pred)
return(c(rmse,mae,mape,smape))
}
#Model 1 (v1, hidden layer =1 , nodes = 6, linear = true, act.fact = tanh)
model1 <- neuralnet(`20H` ~ t1+t2+t3+t4,data = v1_train, hidden = 6, linear.output = T)
library(neuralnet)
model1 <- neuralnet(`20H` ~ t1+t2+t3+t4,data = v1_train, hidden = 6, linear.output = T)
library(xlsx)
loadfile <- read.xlsx(file = 'uow_consumption.xlsx', sheetIndex = 1, header = TRUE)[4:7]
colnames(loadfile)<-c("Date","18H","19H","20H")
dataset <- as.data.frame(loadfile$`20H`)
str(dataset)
#Creating lags
t1 <- lag(dataset,1)
#Get the 20h column
dataset <- as.data.frame(loadfile$`20H`)
#Creating lags
t1 <- lag(dataset,1)
loadfile <- read.xlsx(file = 'uow_consumption.xlsx', sheetIndex = 1, header = TRUE)[4:7]
colnames(loadfile)<-c("Date","18H","19H","20H")
rm(list = ls())
library(xlsx)
library(neuralnet)
library(Metrics)
loadfile <- read.xlsx(file = 'uow_consumption.xlsx', sheetIndex = 1, header = TRUE)[4:7]
colnames(loadfile)<-c("Date","18H","19H","20H")
View(loadfile)
dataset <- as.data.frame(loadfile$`20H`)
str(dataset)
#Creating lags
t1 <- lag(dataset,1)
View(dataset)
t2 <- lag(dataset,2)
#Creating lags
t1 <- lag(1,dataset)
#Creating lags
t1 <- lag(dataset,1)
library(stats)
t1 <- lag(dataset,1)
library(dplyr)
#Creating lags
t1 <- lag(dataset,1)
t2 <- lag(dataset,2)
t3 <- lag(dataset,3)
t4 <- lag(dataset,4)
t7 <- lag(dataset,7)
v1 <- cbind(dataset,t1,t2)
v2 <- cbind(dataset,t1,t2,t3)
v3 <- cbind(dataset,t1,t2,t3,t4)
v4 <- cbind(dataset,t1,t2,t3,t4,t7)
#Remove NA
v1 <- v1[complete.cases(v1),]
v2 <- v2[complete.cases(v2),]
v3 <- v3[complete.cases(v3),]
v4 <- v4[complete.cases(v4),]
colnames(v1) <- c("20H","t-1","t-2")
colnames(v2) <- c("20H","t-1","t-2","t-3")
colnames(v3) <- c("20H","t-1","t-2","t-3","t-4")
colnames(v4) <- c("20H","t-1","t-2","t-3","t-4","t-7")
#normalize function
normalize <- function(x){
return((x-min(x))/(max(x)-min(x)))
}
#normalize matrices
v1_norm <- normalize(v1)
v2_norm <- normalize(v2)
v3_norm <- normalize(v3)
v4_norm <- normalize(v4)
#split train data and test data
v1_train <- v1_norm[1:380,]
v2_train <- v2_norm[1:380,]
v3_train <- v3_norm[1:380,]
v4_train <- v4_norm[1:380,]
v1_test <- v1_norm[381:468,]
v2_test <- v2_norm[381:467,]
v3_test <- v3_norm[381:466,]
v4_test <- v4_norm[381:463,]
#de-normalize function
unnomarlize <- function(x,min,max){
return((max-min)*x+min)
#Actual result and predicted result table function
actual_predict_table <- function(actual,predicted){
act_pred_table <- cbind(actual,predicted)
colnames(act_pred_table) <- c("Expected","Predicted")
return(act_pred_table)
}
#Evaluation function
evaluateResult <- function(actual,pred){
rmse <- rmse(actual = actual,predicted = pred)
mae <- mae(actual = actual, predicted = pred)
mape <- mape(actual = actual, predicted = pred)
smape <- smape(actual = actual, predicted = pred)
return(c(rmse,mae,mape,smape))
}
model1 <- neuralnet(`20H` ~ t1+t2+t3+t4,data = v1_train, hidden = 6, linear.output = T)
#display network diagram
plot(model1)
#display network diagram
plot(model1)
#Evaluate Model
model1_result <- compute(model1,v1_test[2:5])
predicted_result1 <- model1_result$net.result
v1_original_cons_train <- v1[1:380,"20H"]
v1_original_cons_test <- v1[381:468,"20H"]
cunsumption_max <- max(v1_original_cons_train)
consumption_min <- min(v1_original_cons_train)
#Install libraries
library(xlsx)
library(neuralnet)
library(Metrics)
library(stats)
library(dplyr)
#Read dataset
loadfile <- read.xlsx(file = 'uow_consumption.xlsx', sheetIndex = 1, header = TRUE)[4:7]
colnames(loadfile)<-c("Date","18H","19H","20H")
#Get the 20h column
dataset <- as.data.frame(loadfile$`20H`)
str(dataset)
#Creating lags
t1 <- lag(dataset,1)
t2 <- lag(dataset,2)
t3 <- lag(dataset,3)
t4 <- lag(dataset,4)
t7 <- lag(dataset,7)
#I/O matrix
v1 <- cbind(dataset,t1,t2)
v2 <- cbind(dataset,t1,t2,t3)
v3 <- cbind(dataset,t1,t2,t3,t4)
v4 <- cbind(dataset,t1,t2,t3,t4,t7)
#Remove NA
v1 <- v1[complete.cases(v1),]
v2 <- v2[complete.cases(v2),]
v3 <- v3[complete.cases(v3),]
v4 <- v4[complete.cases(v4),]
colnames(v1) <- c("20H","t-1","t-2")
colnames(v2) <- c("20H","t-1","t-2","t-3")
colnames(v3) <- c("20H","t-1","t-2","t-3","t-4")
colnames(v4) <- c("20H","t-1","t-2","t-3","t-4","t-7")
#normalize function
normalize <- function(x){
return((x-min(x))/(max(x)-min(x)))
}
#normalize matrices
v1_norm <- normalize(v1)
v2_norm <- normalize(v2)
v3_norm <- normalize(v3)
v4_norm <- normalize(v4)
#split train data and test data
v1_train <- v1_norm[1:380,]
v2_train <- v2_norm[1:380,]
v3_train <- v3_norm[1:380,]
v4_train <- v4_norm[1:380,]
v1_test <- v1_norm[381:468,]
v2_test <- v2_norm[381:467,]
v3_test <- v3_norm[381:466,]
v4_test <- v4_norm[381:463,]
#de-normalize function
unnomarlize <- function(x,min,max){
return((max-min)*x+min)
}
#Actual result and predicted result table function
actual_predict_table <- function(actual,predicted){
act_pred_table <- cbind(actual,predicted)
colnames(act_pred_table) <- c("Expected","Predicted")
return(act_pred_table)
}
#Evaluation function
evaluateResult <- function(actual,pred){
rmse <- rmse(actual = actual,predicted = pred)
mae <- mae(actual = actual, predicted = pred)
mape <- mape(actual = actual, predicted = pred)
smape <- smape(actual = actual, predicted = pred)
return(c(rmse,mae,mape,smape))
}
#display network diagram
plot(model1)
rm(list = ls())
#Load the data set
#install.packages('xlsx')
library(xlsx)
loadfile <- read.xlsx(file = 'vehicles.xlsx', sheetIndex = 1, header = TRUE)
Dataset <- loadfile[2:20]
#Detect Missing values
sum(is.na(Dataset))
#No missing data
#Detect outliers
boxplot(Dataset[1:18])
#Outliers founded columns
OutlierList <- c(
"Skew.maxis",
"Pr.Axis.Ra",
"Sc.Var.Maxis",
"Rad.Ra",
"Kurt.maxis",
"Skew.maxis",
"Max.L.Ra",
"Sc.Var.maxis"
)
#removing outliers
for (Var in names(Dataset)) {
if (Var %in% names(Dataset[, OutlierList])) {
outliers <- boxplot.stats(Dataset[,Var])$out
Dataset <- subset(Dataset, !Dataset[,Var] %in% outliers)
}
}
#Check still there is any outlier
boxplot(Dataset[1:18])
#there is out liers still and redo the process
#new outlier list
OutlierList2 <- c(
"Skew.Maxis",
"Sc.Var.maxis"
)
#removing remaining outliers
for (Var in names(Dataset)) {
if (Var %in% names(Dataset[, OutlierList2])) {
outliers <- boxplot.stats(Dataset[,Var])$out
Dataset <- subset(Dataset, !Dataset[,Var] %in% outliers)
}
}
#Check still there is any outlier
boxplot(Dataset[1:18])
dataset.cleared <- Dataset[1:18]
label <- Dataset$Class
#Scaling the Dataset
dataset.scaled <- scale(dataset.cleared)
#scaled dataset visualization
boxplot(dataset.scaled)
#Find the optimal number of clusters
install.packages('factoextra')
library(factoextra)
#install.packages('NbClust')
#library(NbClust)
#NbClust
set.seed(23)
no_of_Cluster <- NbClust(dataset.scaled,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
#k =3
#Elbow method
fviz_nbclust(dataset.scaled, kmeans, method = 'wss')
#k =3
#silhouette method
fviz_nbclust(dataset.scaled, kmeans, method = 'silhouette')
#k =2
#Gap statistics
fviz_nbclust(dataset.scaled, kmeans, method = 'gap_stat')
#k =3
### Clustering Attempt k=3
cluster_output <- kmeans(dataset.scaled,3)
cluster_output
#total WSS
wss <- cluster_output$tot.withinss
wss
#BSS
bss <- cluster_output$betweenss
bss
#Clusters
table(label,cluster_output$cluster)
#Centers
cluster_output$centers
#install.packages('fpc')
library(fpc)
plotcluster(dataset.scaled,cluster_output$cluster)
#silhouette plot
library(cluster)
sil <- silhouette(cluster_output$cluster, dist(dataset.scaled))
fviz_silhouette(sil)
#silhouette average width is 0.29
#PCA
cor_val <- cor(dataset.cleared)
cor_val
mean(cor_val)
#perform PCA
pca <- prcomp(dataset.cleared, center = TRUE, scale = TRUE)
summary(pca)
#Eigenvalues
eigenvalues <- pca$sdev^2
#Eigenvectors
eigenvectors <- pca$rotation
eigenvalues
eigenvectors
#cumlative score
cumulative_scores <- cumsum(eigenvalues / sum(eigenvalues) * 100)
vehicles_transformed <- predict(pca, dataset.cleared)
colnames(vehicles_transformed) <- paste0("PC", 1:ncol(vehicles_transformed))
vehicles_transformed
pc_index <- min(which(cumulative_scores > 92))
pcs <- 1:pc_index
pcs
dataset.transformed <- as.data.frame(-pca$x[,1:6])
#NbClust
set.seed(23)
no_of_Cluster <- NbClust(dataset.transformed,distance = 'euclidean',min.nc = 2, max.nc = 10, method = 'kmeans', index = 'all')
#k =2
#Elbow method
fviz_nbclust(dataset.transformed, kmeans, method = 'wss')
#k =2
#silhouette method
fviz_nbclust(dataset.transformed, kmeans, method = 'silhouette')
#k =2
#Gap statistics
fviz_nbclust(dataset.transformed, kmeans, method = 'gap_stat')
#k =3
### Clustering Attempt for new datasetk=2
new_kmeans <- kmeans(dataset.transformed,2)
new_kmeans
#total WSS
new_wss <- new_kmeans$tot.withinss
new_wss
#BSS
new_bss <- new_kmeans$betweenss
new_bss
#Clusters
table(label,new_kmeans$cluster)
#Centers
new_kmeans$centers
plotcluster(dataset.transformed,new_kmeans$cluster)
#silhouette plot
new_sil <- silhouette(new_kmeans$cluster, dist(dataset.transformed))
fviz_silhouette(new_sil)
#silhouette average width is 0.4
